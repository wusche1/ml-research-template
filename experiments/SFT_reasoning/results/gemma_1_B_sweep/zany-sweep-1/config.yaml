function_args: []
function_kwargs:
  dataset_config:
    max_cot_tokens: 1000
    n_eval: 100
  evaluator_config:
    batch_size: 4
    max_new_tokens: 1024
    n_questions: 16
  lora_config:
    bias: none
    lora_alpha: 64
    lora_dropout: 0.05
    r: 32
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    task_type: CAUSAL_LM
  lora_config.lora_alpha: null
  lora_config.r: 16
  model_config:
    device_map: auto
    pretrained_model_name_or_path: unsloth/gemma-3-1b-it
  output_dir: results/gemma_1_B_sweep/zany-sweep-1
  run_name: zany-sweep-1
  saving:
    hf: false
  training.n_eval: 1000
  training_config:
    eval_strategy: epoch
    gradient_accumulation_steps: 1
    learning_rate: 2.0e-05
    logging_steps: 10
    lr_scheduler_type: cosine
    max_length: 2048
    max_steps: 250
    num_train_epochs: 1
    per_device_train_batch_size: 8
    report_to: wandb
    save_only_model: true
    warmup_ratio: 0.1
  training_config.learning_rate: 6.906870335868604e-05
  training_config.max_steps: 1000
  training_config.warmup_ratio: 0.1659284509618718
function_name: train_sft
log_file_path: results/gemma_1_B_sweep/zany-sweep-1/logs.txt
name: zany-sweep-1
save_config_path: results/gemma_1_B_sweep/zany-sweep-1/config.yaml
wandb_entity: null
wandb_group: SFT_reasoning
wandb_project: null
wandb_tags: null
